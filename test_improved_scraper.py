#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦æ”¹é€²å¾Œçš„é«˜é›„ç¾ç”²ç¾ç«åº—å®¶çˆ¬èŸ²
ä¸»è¦æ¸¬è©¦ï¼š
1. é¿å…é‡è¤‡æŠ“å–
2. é¿å…è·³éåº—å®¶
3. ç²¾ç¢ºçš„æ»¾å‹•ä½ç½®æ§åˆ¶
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from google_maps_scraper_kaohsiung_precision import KaohsiungPrecisionScraper

def test_improved_scraper():
    """æ¸¬è©¦æ”¹é€²å¾Œçš„çˆ¬èŸ²"""
    print("ğŸ§ª æ¸¬è©¦æ”¹é€²å¾Œçš„é«˜é›„ç¾ç”²ç¾ç«åº—å®¶çˆ¬èŸ²")
    print("=" * 60)
    print("ğŸ¯ æ¸¬è©¦ç›®æ¨™ï¼š")
    print("   1. é¿å…é‡è¤‡æŠ“å–åŒä¸€å®¶åº—")
    print("   2. é¿å…è·³éåº—å®¶")
    print("   3. ç²¾ç¢ºçš„æ»¾å‹•ä½ç½®æ§åˆ¶")
    print("   4. å·¦å´æœå°‹çµæœä½ç½®ç©©å®š")
    print()
    
    # è©¢å•æ¸¬è©¦æ¨¡å¼
    print("ğŸ”§ æ¸¬è©¦æ¨¡å¼é¸æ“‡ï¼š")
    print("   1. å¿«é€Ÿæ¸¬è©¦ (3å€‹åœ°æ¨™ï¼Œæ¯å€‹åœ°æ¨™æœ€å¤š10å®¶åº—)")
    print("   2. ä¸­ç­‰æ¸¬è©¦ (10å€‹åœ°æ¨™ï¼Œæ¯å€‹åœ°æ¨™æœ€å¤š20å®¶åº—)")
    print("   3. å®Œæ•´æ¸¬è©¦ (æ‰€æœ‰åœ°æ¨™ï¼Œç›®æ¨™2000å®¶åº—)")
    print()
    
    while True:
        choice = input("è«‹é¸æ“‡æ¸¬è©¦æ¨¡å¼ (1/2/3): ").strip()
        if choice in ['1', '2', '3']:
            break
        print("âŒ è«‹è¼¸å…¥ 1ã€2 æˆ– 3")
    
    # è©¢å•ç€è¦½å™¨æ¨¡å¼
    print()
    print("ğŸ–¥ï¸ ç€è¦½å™¨æ¨¡å¼ï¼š")
    print("   1. é¡¯ç¤ºè¦–çª— (æ¨è–¦ï¼Œå¯è§€å¯Ÿæ”¹é€²æ•ˆæœ)")
    print("   2. ç„¡é ­æ¨¡å¼")
    print()
    
    while True:
        browser_choice = input("è«‹é¸æ“‡ç€è¦½å™¨æ¨¡å¼ (1/2): ").strip()
        if browser_choice in ['1', '2']:
            break
        print("âŒ è«‹è¼¸å…¥ 1 æˆ– 2")
    
    show_browser = browser_choice == '1'
    
    # è¨­å®šæ¸¬è©¦åƒæ•¸
    if choice == '1':
        test_landmarks = ["é«˜é›„ç«è»Šç«™", "å¤¢æ™‚ä»£è³¼ç‰©ä¸­å¿ƒ", "å…­åˆå¤œå¸‚"]
        target_shops = 30
        test_name = "å¿«é€Ÿæ¸¬è©¦"
    elif choice == '2':
        test_landmarks = [
            "é«˜é›„ç«è»Šç«™", "å¤¢æ™‚ä»£è³¼ç‰©ä¸­å¿ƒ", "å…­åˆå¤œå¸‚", "æ–°å´›æ±Ÿå•†åœˆ", "æ–‡åŒ–ä¸­å¿ƒ",
            "å·¦ç‡Ÿé«˜éµç«™", "é³³å±±ç«è»Šç«™", "ä¸‰å¤šå•†åœˆ", "ç¾éº—å³¶ç«™", "å·¨è›‹å•†åœˆ"
        ]
        target_shops = 100
        test_name = "ä¸­ç­‰æ¸¬è©¦"
    else:
        test_landmarks = None  # ä½¿ç”¨æ‰€æœ‰åœ°æ¨™
        target_shops = 2000
        test_name = "å®Œæ•´æ¸¬è©¦"
    
    print()
    print(f"ğŸš€ é–‹å§‹ {test_name}")
    print(f"ğŸ¯ ç›®æ¨™åº—å®¶æ•¸: {target_shops}")
    if test_landmarks:
        print(f"ğŸ“ æ¸¬è©¦åœ°æ¨™: {len(test_landmarks)} å€‹")
    else:
        print(f"ğŸ“ æ¸¬è©¦åœ°æ¨™: å…¨éƒ¨åœ°æ¨™")
    print("=" * 60)
    
    # å‰µå»ºçˆ¬èŸ²å¯¦ä¾‹
    scraper = KaohsiungPrecisionScraper(debug_mode=True, show_browser=show_browser)
    
    # å¦‚æœæ˜¯æ¸¬è©¦æ¨¡å¼ï¼Œä¿®æ”¹ç›®æ¨™å’Œåœ°æ¨™
    if test_landmarks:
        scraper.target_shops = target_shops
        # æš«æ™‚ä¿®æ”¹åœ°æ¨™åˆ—è¡¨
        original_get_landmarks = scraper.get_kaohsiung_landmarks
        scraper.get_kaohsiung_landmarks = lambda: test_landmarks
    
    try:
        # åŸ·è¡Œæ¸¬è©¦
        success = scraper.run_precision_scraping()
        
        print()
        print("=" * 60)
        print(f"ğŸ§ª {test_name} å®Œæˆ")
        
        if success and scraper.shops_data:
            print(f"âœ… æ¸¬è©¦æˆåŠŸï¼")
            print(f"ğŸ“Š æ”¶é›†åº—å®¶æ•¸: {len(scraper.shops_data)}")
            print(f"ğŸ¯ ç›®æ¨™é”æˆç‡: {len(scraper.shops_data)/target_shops*100:.1f}%")
            
            # æª¢æŸ¥é‡è¤‡
            unique_names = set()
            unique_urls = set()
            duplicates = 0
            
            for shop in scraper.shops_data:
                name = shop['name'].lower().strip()
                url = shop.get('google_maps_url', '').strip()
                
                if name in unique_names or url in unique_urls:
                    duplicates += 1
                else:
                    unique_names.add(name)
                    unique_urls.add(url)
            
            print(f"ğŸ” é‡è¤‡æª¢æŸ¥: {duplicates} å€‹é‡è¤‡é …ç›®")
            if duplicates == 0:
                print("âœ… ç„¡é‡è¤‡ï¼Œå»é‡åŠŸèƒ½æ­£å¸¸ï¼")
            else:
                print(f"âš ï¸ ç™¼ç¾ {duplicates} å€‹é‡è¤‡é …ç›®ï¼Œéœ€è¦é€²ä¸€æ­¥å„ªåŒ–")
            
            # çµ±è¨ˆè¯çµ¡è³‡è¨Š
            has_address = sum(1 for shop in scraper.shops_data if shop.get('address', 'åœ°å€æœªæä¾›') not in ['åœ°å€æœªæä¾›', 'åœ°å€ç²å–å¤±æ•—'])
            has_phone = sum(1 for shop in scraper.shops_data if shop.get('phone', 'é›»è©±æœªæä¾›') not in ['é›»è©±æœªæä¾›', 'é›»è©±ç²å–å¤±æ•—'])
            has_line = sum(1 for shop in scraper.shops_data if shop.get('line_contact', 'LINEæœªæä¾›') not in ['LINEæœªæä¾›', 'LINEç²å–å¤±æ•—'])
            
            print(f"ğŸ“ è¯çµ¡è³‡è¨Šçµ±è¨ˆ:")
            print(f"   ğŸ“ åœ°å€: {has_address}/{len(scraper.shops_data)} ({has_address/len(scraper.shops_data)*100:.1f}%)")
            print(f"   ğŸ“ é›»è©±: {has_phone}/{len(scraper.shops_data)} ({has_phone/len(scraper.shops_data)*100:.1f}%)")
            print(f"   ğŸ“± LINE: {has_line}/{len(scraper.shops_data)} ({has_line/len(scraper.shops_data)*100:.1f}%)")
            
        else:
            print("âŒ æ¸¬è©¦å¤±æ•—æˆ–æœªæ”¶é›†åˆ°è³‡æ–™")
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ä¸­å‡ºéŒ¯: {e}")
        return False
    
    return True

if __name__ == "__main__":
    test_improved_scraper() 