#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜é›„ç¾ç”²ç¾ç«åº—å®¶å¤šæºæ•´åˆçˆ¬èŸ²
æ•´åˆå¤šå€‹å¹³å°ï¼šGoogleæœå°‹ã€Facebookã€å•†æ¥­ç›®éŒ„ã€æ±‚è·ç¶²ç«™ç­‰
ç›®æ¨™ï¼šå¿«é€Ÿæ”¶é›†å¤§é‡é«˜é›„ç¾ç”²ç¾ç«åº—å®¶åŸºæœ¬è³‡æ–™
ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2024
"""

import time
import random
import pandas as pd
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.firefox.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import logging
from datetime import datetime
import re
import urllib.parse
import json
from threading import Lock
import concurrent.futures

# ç¢ºä¿å®‰è£å¿…è¦å¥—ä»¶
try:
    import openpyxl
    from bs4 import BeautifulSoup
except ImportError:
    print("âš ï¸ æ­£åœ¨å®‰è£å¿…è¦å¥—ä»¶...")
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "openpyxl", "beautifulsoup4", "lxml", "requests"])
    import openpyxl
    from bs4 import BeautifulSoup

class MultiSourceKaohsiungScraper:
    def __init__(self, debug_mode=True, show_browser=False):
        self.debug_mode = debug_mode
        self.show_browser = show_browser
        self.setup_logging()
        self.driver = None
        self.shops_data = []
        self.target_shops = 2000
        self.data_lock = Lock()
        
        # æœå°‹é—œéµå­—
        self.beauty_keywords = [
            "ç¾ç”²", "ç¾ç«", "è€³ç‡­", "æ¡è€³", "ç†±è Ÿ", "ç¾å®¹", "ç¾é«”", 
            "æŒ‡ç”²å½©ç¹ª", "ç«æ¯›å«æ¥", "ç¾ç”²å·¥ä½œå®¤", "ç¾ç«å·¥ä½œå®¤",
            "nail", "eyelash", "ç¾ç”²åº—", "ç¾ç«åº—", "ç¾å®¹é™¢"
        ]
        
        # é«˜é›„åœ°å€
        self.kaohsiung_areas = [
            "é«˜é›„å¸‚", "é«˜é›„", "é³³å±±", "å·¦ç‡Ÿ", "æ¥ æ¢“", "ä¸‰æ°‘", "è‹“é›…", 
            "æ–°èˆˆ", "å‰é‡‘", "é¼“å±±", "å‰é®", "å°æ¸¯", "ä»æ­¦", "å¤§ç¤¾", 
            "å²¡å±±", "è·¯ç«¹", "æ©‹é ­", "ç‡•å·¢", "å¤§æ¨¹", "å¤§å¯®", "æ—åœ’", 
            "é³¥æ¾", "æ——å±±", "ç¾æ¿ƒ"
        ]
        
        # ç”¨æˆ¶ä»£ç†
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0"
        ]
        
    def setup_logging(self):
        """è¨­å®šæ—¥èªŒ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('multi_source_scraper.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def debug_print(self, message, level="INFO"):
        """è¼¸å‡ºè¨Šæ¯"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        symbols = {
            "INFO": "â„¹ï¸", "SUCCESS": "âœ…", "ERROR": "âŒ", "WARNING": "âš ï¸",
            "EXTRACT": "ğŸ”", "SAVE": "ğŸ’¾", "TARGET": "ğŸ¯", "PLATFORM": "ğŸŒ"
        }
        symbol = symbols.get(level, "ğŸ“‹")
        print(f"[{timestamp}] {symbol} {message}")
        if self.debug_mode:
            self.logger.info(f"{level}: {message}")
    
    def setup_driver(self):
        """è¨­å®šç€è¦½å™¨"""
        try:
            self.debug_print("ğŸ¦Š è¨­å®šFirefoxç€è¦½å™¨...", "INFO")
            
            options = Options()
            if not self.show_browser:
                options.add_argument("--headless")
            
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-gpu")
            options.add_argument("--disable-extensions")
            options.add_argument("--width=1920")
            options.add_argument("--height=1080")
            
            # æ€§èƒ½å„ªåŒ–
            prefs = {
                "permissions.default.image": 2,
                "dom.webnotifications.enabled": False,
                "media.autoplay.enabled": False,
                "general.useragent.override": random.choice(self.user_agents)
            }
            
            for key, value in prefs.items():
                options.set_preference(key, value)
            
            options.log.level = "fatal"
            
            self.driver = webdriver.Firefox(options=options)
            self.driver.set_window_size(1920, 1080)
            
            self.debug_print("âœ… Firefoxè¨­å®šå®Œæˆ", "SUCCESS")
            return True
            
        except Exception as e:
            self.debug_print(f"âŒ Firefoxè¨­å®šå¤±æ•—: {e}", "ERROR")
            return False
    
    def get_session(self):
        """ç²å–HTTPæœƒè©±"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        })
        return session
    
    def add_shop_data(self, shop_info):
        """å®‰å…¨æ·»åŠ åº—å®¶è³‡æ–™"""
        with self.data_lock:
            # æª¢æŸ¥é‡è¤‡
            for existing in self.shops_data:
                if (existing['name'].lower().strip() == shop_info['name'].lower().strip() or
                    (existing.get('phone', '') == shop_info.get('phone', '') and 
                     shop_info.get('phone', '') not in ['', 'éœ€é€²ä¸€æ­¥æŸ¥è©¢'])):
                    return False
            
            self.shops_data.append(shop_info)
            self.debug_print(f"âœ… æ–°å¢åº—å®¶ ({len(self.shops_data)}): {shop_info['name']}", "SUCCESS")
            return True
    
    def scrape_google_search(self, keyword, area):
        """Googleæœå°‹"""
        try:
            self.debug_print(f"ğŸ” Googleæœå°‹: {keyword} {area}", "PLATFORM")
            
            if not self.driver:
                if not self.setup_driver():
                    return []
            
            search_query = f"{keyword} {area} åº—å®¶ é›»è©± åœ°å€"
            search_url = f"https://www.google.com/search?q={urllib.parse.quote(search_query)}"
            
            self.driver.get(search_url)
            time.sleep(3)
            
            shops = []
            
            # æå–æœå°‹çµæœ
            results = self.driver.find_elements(By.CSS_SELECTOR, "div.g")[:15]
            
            for result in results:
                try:
                    # æ¨™é¡Œ
                    title_elem = result.find_element(By.CSS_SELECTOR, "h3")
                    title = title_elem.text.strip() if title_elem else ""
                    
                    if not title or len(title) < 3:
                        continue
                    
                    # æª¢æŸ¥ç›¸é—œæ€§
                    if not any(kw in title.lower() for kw in ['ç¾ç”²', 'ç¾ç«', 'ç¾å®¹', 'æŒ‡ç”²', 'ç«æ¯›']):
                        continue
                    
                    # æè¿°æ–‡å­—
                    desc_elem = result.find_element(By.CSS_SELECTOR, "span")
                    desc_text = desc_elem.text if desc_elem else ""
                    
                    # æå–é›»è©±
                    phone_match = re.search(r'0\d{1,2}[-\s]?\d{6,8}|09\d{8}', desc_text)
                    phone = phone_match.group() if phone_match else 'éœ€é€²ä¸€æ­¥æŸ¥è©¢'
                    
                    # æå–åœ°å€
                    addr_match = re.search(r'é«˜é›„[^,\n]{5,40}', desc_text)
                    address = addr_match.group() if addr_match else f'{area}ï¼ˆè©³ç´°åœ°å€éœ€é€²ä¸€æ­¥æŸ¥è©¢ï¼‰'
                    
                    shop_info = {
                        'name': title,
                        'address': address,
                        'phone': phone,
                        'line_contact': 'éœ€é€²ä¸€æ­¥æŸ¥è©¢',
                        'source': 'Googleæœå°‹',
                        'google_maps_url': ''
                    }
                    
                    shops.append(shop_info)
                    
                except Exception as e:
                    continue
            
            self.debug_print(f"ğŸ“Š Googleæœå°‹æ‰¾åˆ° {len(shops)} å®¶åº—", "SUCCESS")
            return shops
            
        except Exception as e:
            self.debug_print(f"âŒ Googleæœå°‹å¤±æ•—: {e}", "ERROR")
            return []
    
    def scrape_business_websites(self, keyword, area):
        """æœå°‹å•†æ¥­ç¶²ç«™"""
        try:
            self.debug_print(f"ğŸ¢ å•†æ¥­ç¶²ç«™æœå°‹: {keyword} {area}", "PLATFORM")
            
            session = self.get_session()
            shops = []
            
            # å•†æ¥­ç¶²ç«™åˆ—è¡¨
            websites = [
                f"https://www.iyp.com.tw/search.html?q={urllib.parse.quote(keyword + ' ' + area)}",
                f"https://www.518.com.tw/job-search-1.html?i=1&am=1&kwop=7&kw={urllib.parse.quote(keyword)}",
                f"https://www.104.com.tw/jobs/search/?keyword={urllib.parse.quote(keyword + ' ' + area)}"
            ]
            
            for url in websites:
                try:
                    response = session.get(url, timeout=15)
                    if response.status_code != 200:
                        continue
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # é€šç”¨å•†å®¶è³‡è¨Šæå–
                    business_elements = soup.find_all(['div', 'li', 'article'], 
                                                    class_=re.compile(r'(business|company|shop|store|job)', re.I))[:20]
                    
                    for elem in business_elements:
                        try:
                            text_content = elem.get_text()
                            
                            # æŸ¥æ‰¾åŒ…å«ç¾ç”²ç¾ç«é—œéµå­—çš„å…§å®¹
                            if any(kw in text_content.lower() for kw in self.beauty_keywords):
                                
                                # æå–åº—å
                                name_elem = elem.find(['h1', 'h2', 'h3', 'h4', 'a', 'strong'])
                                name = name_elem.get_text().strip() if name_elem else ""
                                
                                if not name or len(name) < 3 or len(name) > 50:
                                    continue
                                
                                # æå–é›»è©±
                                phone_match = re.search(r'0\d{1,2}[-\s]?\d{6,8}|09\d{8}', text_content)
                                phone = phone_match.group() if phone_match else 'éœ€é€²ä¸€æ­¥æŸ¥è©¢'
                                
                                # æå–åœ°å€
                                addr_match = re.search(r'é«˜é›„[å¸‚]?[^,\n]{5,40}', text_content)
                                address = addr_match.group() if addr_match else f'{area}ï¼ˆè©³ç´°åœ°å€éœ€é€²ä¸€æ­¥æŸ¥è©¢ï¼‰'
                                
                                shop_info = {
                                    'name': name,
                                    'address': address,
                                    'phone': phone,
                                    'line_contact': 'éœ€é€²ä¸€æ­¥æŸ¥è©¢',
                                    'source': 'å•†æ¥­ç¶²ç«™',
                                    'google_maps_url': ''
                                }
                                
                                shops.append(shop_info)
                                
                        except Exception as e:
                            continue
                            
                except Exception as e:
                    self.debug_print(f"âš ï¸ ç¶²ç«™ {url} æœå°‹å¤±æ•—: {e}", "WARNING")
                    continue
                
                # é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
                time.sleep(random.uniform(1, 3))
            
            self.debug_print(f"ğŸ“Š å•†æ¥­ç¶²ç«™æ‰¾åˆ° {len(shops)} å®¶åº—", "SUCCESS")
            return shops
            
        except Exception as e:
            self.debug_print(f"âŒ å•†æ¥­ç¶²ç«™æœå°‹å¤±æ•—: {e}", "ERROR")
            return []
    
    def scrape_social_media(self, keyword, area):
        """æœå°‹ç¤¾ç¾¤åª’é«”"""
        try:
            self.debug_print(f"ğŸ“± ç¤¾ç¾¤åª’é«”æœå°‹: {keyword} {area}", "PLATFORM")
            
            if not self.driver:
                if not self.setup_driver():
                    return []
            
            shops = []
            
            # Facebookæœå°‹
            try:
                search_query = f"{keyword} {area}"
                fb_url = f"https://www.facebook.com/search/pages/?q={urllib.parse.quote(search_query)}"
                
                self.driver.get(fb_url)
                time.sleep(5)
                
                # æå–Facebooké é¢
                page_elements = self.driver.find_elements(By.CSS_SELECTOR, "[role='article']")[:10]
                
                for elem in page_elements:
                    try:
                        text_content = elem.text
                        
                        # æŸ¥æ‰¾åº—å
                        name_lines = text_content.split('\n')
                        for line in name_lines:
                            if any(kw in line.lower() for kw in self.beauty_keywords) and len(line) < 50:
                                name = line.strip()
                                
                                # æå–è¯çµ¡è³‡è¨Š
                                phone_match = re.search(r'0\d{1,2}[-\s]?\d{6,8}|09\d{8}', text_content)
                                phone = phone_match.group() if phone_match else 'éœ€é€²ä¸€æ­¥æŸ¥è©¢'
                                
                                addr_match = re.search(r'é«˜é›„[^,\n]{5,40}', text_content)
                                address = addr_match.group() if addr_match else f'{area}ï¼ˆè©³ç´°åœ°å€éœ€é€²ä¸€æ­¥æŸ¥è©¢ï¼‰'
                                
                                shop_info = {
                                    'name': name,
                                    'address': address,
                                    'phone': phone,
                                    'line_contact': 'å¯èƒ½æœ‰LINEï¼Œéœ€é€²ä¸€æ­¥æŸ¥è©¢',
                                    'source': 'Facebook',
                                    'google_maps_url': ''
                                }
                                
                                shops.append(shop_info)
                                break
                                
                    except Exception as e:
                        continue
                        
            except Exception as e:
                self.debug_print(f"âš ï¸ Facebookæœå°‹å¤±æ•—: {e}", "WARNING")
            
            self.debug_print(f"ğŸ“Š ç¤¾ç¾¤åª’é«”æ‰¾åˆ° {len(shops)} å®¶åº—", "SUCCESS")
            return shops
            
        except Exception as e:
            self.debug_print(f"âŒ ç¤¾ç¾¤åª’é«”æœå°‹å¤±æ•—: {e}", "ERROR")
            return []
    
    def scrape_directory_sites(self, keyword, area):
        """æœå°‹ç›®éŒ„ç¶²ç«™"""
        try:
            self.debug_print(f"ğŸ“‹ ç›®éŒ„ç¶²ç«™æœå°‹: {keyword} {area}", "PLATFORM")
            
            session = self.get_session()
            shops = []
            
            # ç›®éŒ„ç¶²ç«™
            directory_sites = [
                f"https://www.lifego.tw/search?q={urllib.parse.quote(keyword + ' ' + area)}",
                f"https://www.walkerland.com.tw/search?q={urllib.parse.quote(keyword + ' ' + area)}",
                f"https://www.gomaji.com/search?keyword={urllib.parse.quote(keyword + ' ' + area)}"
            ]
            
            for site_url in directory_sites:
                try:
                    response = session.get(site_url, timeout=15)
                    if response.status_code != 200:
                        continue
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # æå–å•†å®¶å¡ç‰‡
                    cards = soup.find_all(['div', 'article', 'section'], 
                                        class_=re.compile(r'(card|item|business|shop|store)', re.I))[:15]
                    
                    for card in cards:
                        try:
                            card_text = card.get_text()
                            
                            # æª¢æŸ¥ç›¸é—œæ€§
                            if not any(kw in card_text.lower() for kw in self.beauty_keywords):
                                continue
                            
                            # æå–åº—å
                            title_elem = card.find(['h1', 'h2', 'h3', 'h4', 'a'])
                            if title_elem:
                                name = title_elem.get_text().strip()
                                
                                if len(name) < 3 or len(name) > 50:
                                    continue
                                
                                # æå–è¯çµ¡è³‡è¨Š
                                phone_match = re.search(r'0\d{1,2}[-\s]?\d{6,8}|09\d{8}', card_text)
                                phone = phone_match.group() if phone_match else 'éœ€é€²ä¸€æ­¥æŸ¥è©¢'
                                
                                addr_match = re.search(r'é«˜é›„[^,\n]{5,40}', card_text)
                                address = addr_match.group() if addr_match else f'{area}ï¼ˆè©³ç´°åœ°å€éœ€é€²ä¸€æ­¥æŸ¥è©¢ï¼‰'
                                
                                shop_info = {
                                    'name': name,
                                    'address': address,
                                    'phone': phone,
                                    'line_contact': 'éœ€é€²ä¸€æ­¥æŸ¥è©¢',
                                    'source': 'ç›®éŒ„ç¶²ç«™',
                                    'google_maps_url': ''
                                }
                                
                                shops.append(shop_info)
                                
                        except Exception as e:
                            continue
                            
                except Exception as e:
                    continue
                
                time.sleep(random.uniform(1, 2))
            
            self.debug_print(f"ğŸ“Š ç›®éŒ„ç¶²ç«™æ‰¾åˆ° {len(shops)} å®¶åº—", "SUCCESS")
            return shops
            
        except Exception as e:
            self.debug_print(f"âŒ ç›®éŒ„ç¶²ç«™æœå°‹å¤±æ•—: {e}", "ERROR")
            return []
    
    def run_multi_source_scraping(self):
        """åŸ·è¡Œå¤šæºæœå°‹"""
        start_time = time.time()
        
        try:
            self.debug_print("ğŸš€ é–‹å§‹å¤šæºæ•´åˆæœå°‹", "INFO")
            self.debug_print(f"ğŸ¯ ç›®æ¨™ï¼š{self.target_shops} å®¶åº—å®¶", "TARGET")
            self.debug_print("ğŸŒ æœå°‹å¹³å°ï¼šGoogleæœå°‹ã€å•†æ¥­ç¶²ç«™ã€ç¤¾ç¾¤åª’é«”ã€ç›®éŒ„ç¶²ç«™", "INFO")
            print("=" * 70)
            
            # æœå°‹ä»»å‹™
            search_tasks = []
            for keyword in self.beauty_keywords[:6]:  # å‰6å€‹é—œéµå­—
                for area in self.kaohsiung_areas[:8]:  # å‰8å€‹åœ°å€
                    search_tasks.append((keyword, area))
            
            self.debug_print(f"ğŸ“‹ æº–å‚™ {len(search_tasks)} å€‹æœå°‹ä»»å‹™", "INFO")
            
            # åŸ·è¡Œæœå°‹
            task_count = 0
            for keyword, area in search_tasks:
                task_count += 1
                progress = (task_count / len(search_tasks)) * 100
                
                self.debug_print(f"[{task_count}/{len(search_tasks)}] æœå°‹: {keyword} @ {area} ({progress:.1f}%)", "INFO")
                
                # å„å¹³å°æœå°‹
                platforms = [
                    ("Googleæœå°‹", self.scrape_google_search),
                    ("å•†æ¥­ç¶²ç«™", self.scrape_business_websites),
                    ("ç¤¾ç¾¤åª’é«”", self.scrape_social_media),
                    ("ç›®éŒ„ç¶²ç«™", self.scrape_directory_sites)
                ]
                
                for platform_name, scrape_func in platforms:
                    try:
                        shops = scrape_func(keyword, area)
                        for shop in shops:
                            if len(self.shops_data) >= self.target_shops:
                                break
                            self.add_shop_data(shop)
                        
                        if len(self.shops_data) >= self.target_shops:
                            self.debug_print(f"ğŸ¯ é”åˆ°ç›®æ¨™ï¼", "TARGET")
                            break
                            
                    except Exception as e:
                        self.debug_print(f"âš ï¸ {platform_name}æœå°‹å¤±æ•—: {e}", "WARNING")
                        continue
                
                if len(self.shops_data) >= self.target_shops:
                    break
                
                # é€²åº¦å ±å‘Š
                if len(self.shops_data) % 100 == 0 and len(self.shops_data) > 0:
                    completion = (len(self.shops_data) / self.target_shops) * 100
                    self.debug_print(f"ğŸ“Š é€²åº¦: {len(self.shops_data)}/{self.target_shops} ({completion:.1f}%)", "INFO")
                
                # æœå°‹é–“éš”
                time.sleep(random.uniform(2, 4))
            
            # å„²å­˜çµæœ
            if self.shops_data:
                self.save_results()
                
                elapsed_time = time.time() - start_time
                minutes = int(elapsed_time // 60)
                seconds = int(elapsed_time % 60)
                
                self.debug_print(f"âœ… æœå°‹å®Œæˆï¼", "SUCCESS")
                self.debug_print(f"ğŸ“Š æ”¶é›†åº—å®¶: {len(self.shops_data)} å®¶", "SUCCESS")
                self.debug_print(f"â±ï¸ åŸ·è¡Œæ™‚é–“: {minutes}åˆ†{seconds}ç§’", "SUCCESS")
                
                # å¹³å°çµ±è¨ˆ
                platform_stats = {}
                for shop in self.shops_data:
                    platform = shop.get('source', 'æœªçŸ¥')
                    platform_stats[platform] = platform_stats.get(platform, 0) + 1
                
                self.debug_print("ğŸ“ˆ å„å¹³å°è²¢ç»:", "INFO")
                for platform, count in platform_stats.items():
                    percentage = (count / len(self.shops_data)) * 100
                    self.debug_print(f"   {platform}: {count} å®¶ ({percentage:.1f}%)", "INFO")
                
            else:
                self.debug_print("âŒ æœªæ”¶é›†åˆ°åº—å®¶è³‡æ–™", "ERROR")
            
            return True
            
        except Exception as e:
            self.debug_print(f"âŒ æœå°‹å¤±æ•—: {e}", "ERROR")
            return False
        
        finally:
            if self.driver:
                try:
                    self.driver.quit()
                except:
                    pass
    
    def save_results(self):
        """å„²å­˜çµæœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"é«˜é›„ç¾ç”²ç¾ç«åº—å®¶_å¤šæºæ•´åˆ_{len(self.shops_data)}å®¶_{timestamp}"
            
            # Excel
            excel_file = f"{filename}.xlsx"
            df = pd.DataFrame(self.shops_data)
            df.to_excel(excel_file, index=False, engine='openpyxl')
            
            # CSV
            csv_file = f"{filename}.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            
            self.debug_print(f"ğŸ’¾ Excel: {excel_file}", "SAVE")
            self.debug_print(f"ğŸ’¾ CSV: {csv_file}", "SAVE")
            
            # çµ±è¨ˆå®Œæ•´æ€§
            complete_phone = sum(1 for shop in self.shops_data 
                               if shop.get('phone', '') not in ['éœ€é€²ä¸€æ­¥æŸ¥è©¢', ''])
            complete_address = sum(1 for shop in self.shops_data 
                                 if 'è©³ç´°åœ°å€éœ€é€²ä¸€æ­¥æŸ¥è©¢' not in shop.get('address', ''))
            
            self.debug_print(f"ğŸ“Š é›»è©±å®Œæ•´æ€§: {complete_phone}/{len(self.shops_data)} ({complete_phone/len(self.shops_data)*100:.1f}%)", "INFO")
            self.debug_print(f"ğŸ“Š åœ°å€å®Œæ•´æ€§: {complete_address}/{len(self.shops_data)} ({complete_address/len(self.shops_data)*100:.1f}%)", "INFO")
            
            return True
            
        except Exception as e:
            self.debug_print(f"âŒ å„²å­˜å¤±æ•—: {e}", "ERROR")
            return False

def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸš€ é«˜é›„ç¾ç”²ç¾ç«åº—å®¶å¤šæºæ•´åˆçˆ¬èŸ²")
    print()
    print("ğŸ¯ æœå°‹ç›®æ¨™ï¼š")
    print("   - æ”¶é›†2000å®¶åº—å®¶è³‡æ–™")
    print("   - åº—åã€åœ°å€ã€é›»è©±ã€LINE")
    print("   - å¤šå¹³å°æ•´åˆæœå°‹")
    print()
    print("ğŸŒ æœå°‹å¹³å°ï¼š")
    print("   - Googleæœå°‹çµæœ")
    print("   - å•†æ¥­ç¶²ç«™ï¼ˆ518ã€104ç­‰ï¼‰")
    print("   - ç¤¾ç¾¤åª’é«”ï¼ˆFacebookï¼‰")
    print("   - ç›®éŒ„ç¶²ç«™ï¼ˆç”Ÿæ´»ç¶²ç«™ï¼‰")
    print()
    print("âš¡ å„ªå‹¢ï¼š")
    print("   - å¤šæºæ•´åˆï¼Œæ•¸é‡æ›´å¤š")
    print("   - é€Ÿåº¦è¼ƒå¿«ï¼Œæ•ˆç‡æ›´é«˜")
    print("   - è‡ªå‹•å»é‡ï¼Œé¿å…é‡è¤‡")
    print()
    
    # ç€è¦½å™¨è¨­å®š
    print("ğŸ–¥ï¸ ç€è¦½å™¨è¨­å®šï¼š")
    print("   1. ç„¡é ­æ¨¡å¼ (æ¨è–¦ï¼Œé€Ÿåº¦å¿«)")
    print("   2. é¡¯ç¤ºè¦–çª— (å¯è§€å¯Ÿé€²åº¦)")
    
    while True:
        choice = input("è«‹é¸æ“‡ (1/2): ").strip()
        if choice == "1":
            show_browser = False
            print("âœ… é¸æ“‡ï¼šç„¡é ­æ¨¡å¼")
            break
        elif choice == "2":
            show_browser = True
            print("âœ… é¸æ“‡ï¼šé¡¯ç¤ºè¦–çª—")
            break
        else:
            print("âŒ è«‹è¼¸å…¥ 1 æˆ– 2")
    
    print("-" * 50)
    confirm = input("ç¢ºå®šé–‹å§‹æœå°‹ï¼Ÿ(y/n): ").strip().lower()
    if confirm != 'y':
        print("ç¨‹å¼å·²å–æ¶ˆ")
        return
    
    scraper = MultiSourceKaohsiungScraper(debug_mode=True, show_browser=show_browser)
    scraper.run_multi_source_scraping()

if __name__ == "__main__":
    main() 